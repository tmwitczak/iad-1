\documentclass{classrep}
\usepackage[utf8]{inputenc}
\frenchspacing

\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\usepackage[hidelinks]{hyperref}
\usepackage{lmodern}
\usepackage{url}
\usepackage{amsmath, amssymb, mathtools}

\usepackage{fancyhdr, lastpage}
\pagestyle{fancyplain}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\cfoot{\thepage\ / \pageref*{LastPage}}


\studycycle{Informatyka, studia dzienne, I st.}
\coursesemester{IV}

\coursename{Inteligentna Analiza Danych}
\courseyear{2018/2019}

\courseteacher{mgr inż. Paweł Tarasiuk}
\coursegroup{poniedziałek, 12:15}

\author{%
  \studentinfo[216806@edu.p.lodz.pl]{Kamil Kowalewski}{216806}\\
  \studentinfo[216920@edu.p.lodz.pl]{Tomasz Witczak}{216920}%
}

\title{Zadanie 1.: Algorytmy klasteryzacji}

\begin{document}
\maketitle
\thispagestyle{fancyplain}

\section{Cel}
{Celem zadania było stworzenie programu implementującego dwa algorytmu klasteryzacji:\\
1. Algorytm k - średnich\\
2. Algorytm samoorganizującej się sieci neuronowej Kohonena\\
3. Algorytm samoorganizującej się sieci neuronowej gazu neuronowego\\\\
Sprawdzenie działania oraz skuteczności algorytmów należało wykonać na trzech zbiorach danych.
}

\section{Wprowadzenie}
{Algorytm k - średnich \\
Należy do grupy algorytmów iteracyjnych oraz analizujących skupienia czyli w czasie kolejnych iteracji wyodrębnia grupy podobnych obiektów. Liczba klas na jakie są dzielone dane wejściowe jest z góry założona. Jedną z najważniejszych elementów jest centroid. Algorytm polega na losowym umiejscowieniu centroidów oraz przenoszeniu ich do centrum skupień punktów. Dane podzbiory są wybierane na podstawie przyporządkowania najbliższego centroidu do punktu. Odległości są obliczane przy pomocy wzoru na odległość Euklidesową. Algorytm zostaje przerwany gdy pozycje centroidów zostaną ustabilizowane.\\\\

Algorytm Kohonena\\
Należy do grupy algorytmów samoorganizujących się sieci neuronowych. Neurony swobodnie poruszają się w N wymiarowej przestrzeni, warto nadmienić, że N to ilość pomiarów dla jednego punktu w zbiorze danych - w przykładowych danych jest to po prostu liczba kolumn. Każdy punkt ze zbioru danych ma przydzielany zwycięski neuron wiec metoda uczenia w tym algorytmie jest konkurencyjna. Zwycięski neuron jest wybierany zgodnie z odległością wektora wagowego od danego wektora punktu. Odległość jest liczona z użyciem wzoru na odległość Euklidesową - jest tak w przypadku dwóch punktów natomiast gdy jest wiecej wymiarów to wzór się rozrasta czyli jest obliczany pierwiastek z sumy kwadratów poszczególnych składowych położenia w przestrzeni. \\

Wzór na odległość Euklidesową dla dwóch punktów: 
\begin{align*}
d=\displaystyle\sqrt{\sum_{i=0}^{i=n} {(X_i - W_i)^2}}
\end{align*}
Wzór na modyfikacje wagi zwycięskiego neuronu: 
\begin{align*}
W_i(t + 1) = W_i(t) + \epsilon * G(d)* (X - W_i(t))
\end{align*}
gdzie:\\
t - liczba iteracji\\
G - wzór na sąsiedztwo Gaussa, wzór poniżej:
\begin{align*}
G(t) = \exp (-\frac{d^2} {2* \delta^2(t)})s
\end{align*}
gdzie:
\begin{align*}
\delta(t) = \delta_0 * \exp (-\frac {t} { \lambda })
\end{align*}
\\
Algorytm gazu neuronowego\\
Należy do grupy algorytmów samoorganizujących się sieci neuronowych. Precyzując w tej metodzie nie ma sieci natomiast jest kolekcja swobodnych neuronów, które poruszają się swobodnie w określonej przestrzeni. Podobnie jak w przypadku algorytmu Kohonena metoda uczenia jest konkurencyjna czyli ograniczenie swobody polega na współzawodnictwie. Gaz neuronowy definiuje sąsiedztwo na podstawie odległości neuronów w przestrzeni wejściowej. \\

Wzór na modyfikacje wartości wag neuronu:
\begin{align*}
W_i(k+1)=W_i(k)+\eta_i(k)G_{(i,x)}(k)[x(k)-W_i(k)]
\end{align*}
gdzie G to funkcja sąsiedztwa:
\begin{align*}
G_{(i,x)}(k)=\exp(-m(i)/\lambda(k))
\end{align*}
gdzie m(i) oznacza numer neuronu i w rankingu, numer 0 oznacza zwycięzcę.\\
Parametr $\lambda$ można rozumieć jako promień sąsiedztwa.

}

\section{Opis implementacji}
{Język użyty do stworzenie programu to Python 3.7.2. Łączy on takie zalety jak świetne biblioteki takie jak numpy, które wykonują obliczenia niskopoziomowo czyli obliczenia są wykonywane w bardzo szybki sposób a jednocześnie składnia jest bardzo czytelna oraz przyjazna dla programisty. W czasie tworzenia programu korzystaliśmy z systemu kontroli wersji git. Cały projekt został podzielony na katalogi:\\
- data\\
- report\\
- src\\
gdzie w data znajdują się zbiory danych do testów, w report znajduję sie sprawozdanie w formacie .tex oraz wygenerowane sprawozdanie w formacie .pdf. Katalog src zawiera kod źródłowy klas napisanych przez nas. Dla ułatwienia obsługi plik main.py, który zawiera funkcje main() nie został dodany do żadnego z powyżej wymienionych katalogów.
	\subsection{Plik  .py}
	{

	}
	\subsection{Plik  .py}
	{

	}
	\subsection{Plik  .py}
	{

	}
	\subsection{Plik  .py}
	{

	}
}

\section{Materiały i metody}
{Aby zapewnić poprawne działanie programu wszystkie wyżej wymienione elementy muszą być obecne. Aby zapewnić większe bezpieczeństwo oraz pewność, że program zawsze zadziała dane testowe są dołączone do programu, również w systemie kontroli wersji katalog data był cały czas obecny. 
}

\section{Wyniki}
{
	\subsection{Zbiór irysów}
	{
		Algorytm k - średnich\\


		Algorytm Kohonena\\


		Algorytm gazu neuronowego\\
	}
	\subsection{Zbiór win}
	{
		Algorytm k - średnich\\


		Algorytm Kohonena\\


		Algorytm gazu neuronowego\\
	}
	\subsection{Zbiór abalone}
	{
		Algorytm k - średnich\\


		Algorytm Kohonena\\


		Algorytm gazu neuronowego\\
	}
}

\section{Dyskusja}
{
	Na podstawie uzyskanych wyników stwierdzamy, że
}

\section{Wnioski}
{
	Podsumowując wykonane zadanie wnioskujemy, że:\\
}

\begin{thebibliography}{0}
  \bibitem{l2short}\url{ http://wikizmsi.zut.edu.pl/uploads/6/6f/InstrukcjaGaz.pdf}
  \bibitem{l2short} \url{https://en.wikipedia.org/wiki/K-means\_clustering}
  \bibitem{l2short} \url{http://www.michalbereta.pl/dydaktyka/WdoSI/lab_neuronowe_II}
  \bibitem{l2short} 
\end{thebibliography}

{

}

\end{document}
